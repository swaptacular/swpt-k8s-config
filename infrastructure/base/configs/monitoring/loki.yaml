apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: loki
spec:
  interval: 10m
  chart:
    spec:
      chart: charts/loki-6.30.1.tgz
      sourceRef:
        kind: GitRepository
        name: flux-system
        namespace: flux-system

  targetNamespace: monitoring
  releaseName: loki
  postRenderers:
    - kustomize:
        images: []
  values:
    global:
      image:
        # Overrides the container registry globally for all images.
        registry: null
      extraArgs:
        - "-config.expand-env=true"
        - "-log.level=warn"
      extraEnvFrom:
        - secretRef:
            name: loki-object-store
      extraVolumes:
        - name: minio-ca-certificate
          configMap:
            name: kube-root-ca.crt
      extraVolumeMounts:
        - name: minio-ca-certificate
          mountPath: /certs/minio
      priorityClassName: critical-monitoring
    imagePullSecrets:
      - name: regcreds
    loki:
      image:
        repository: grafana/loki
        tag: 3.5.1
        digest: null
      auth_enabled: false
      commonConfig:
        replication_factor: 1  # This is 3 by default, and should be 3 in production.
      schemaConfig:
        configs:
          - from: "2024-04-01"
            store: tsdb
            object_store: s3
            schema: v13
            index:
              prefix: loki_index_
              period: 24h
      storage:
        type: s3
        s3:
          endpoint: "${AWS_ENDPOINT}"
          region: "${AWS_REGION}"
          accessKeyId: "${AWS_ACCESS_KEY_ID}"
          secretAccessKey: "${AWS_SECRET_ACCESS_KEY}"
          s3ForcePathStyle: true
          insecure: false
          http_config:
            # This specifies the trusted CA for the local MinIO S3
            # server. This should not be necessary for "proper" S3
            # providers.
            ca_file: /certs/minio/ca.crt
        bucketNames:
          chunks: loki-chunks
          ruler: loki-ruler
          admin: loki-admin
      ingester:
        lifecycler:
          ring:
            # https://grafana.com/blog/2021/02/16/the-essential-config-settings-you-should-use-so-you-wont-drop-logs-in-loki/
            # suggest that `heartbeat_timeout` should be increased to
            # 10 miniutes (the default is 1m). Note that the similar
            # setting `ingester.lifecycler.heartbeat_timeout` seems to
            # be deprecated, but this is not mentioned in the docs.
            # However, ChatGPT "claims" that 3m is a sweet spot when
            # using memberlist kvstore with non-HA control plane.
            heartbeat_timeout: 3m
        chunk_idle_period: 1h
        chunk_encoding: snappy
        wal:
          # Maximum memory size the WAL may use during replay. After
          # hitting this, it will flush data to storage before
          # continuing. A unit suffix (KB, MB, GB) may be applied. The
          # default is 4GB. Normally, this should be about 75% of the
          # memory request for the ingester pods.
          replay_memory_ceiling: 100MB
      querier:
        # Default is 4, if you have enough memory and CPU you can
        # increase, reduce if OOMing.
        max_concurrent: 4
      compactor:
        retention_enabled: true
        delete_request_store: s3
      limits_config:
        retention_period: 30d
        max_query_lookback: 30d  # Should be the same as `retention_period`.
        volume_enabled: true
    write:
      replicas: 1
      persistence:
        size: 1Gi  # Should be at least 10Gi in production, probably quite a bit more.
        storageClass: null
      resources: {}
    read:
      replicas: 1
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: read
              topologyKey: kubernetes.io/hostname
              matchLabelKeys:
              - pod-template-hash
      resources: {}
    backend:
      replicas: 1
      persistence:
        size: 1Gi  # The default is 10Gi.
        storageClass: null
      resources: {}
    chunksCache:
      replicas: 1
      allocatedMemory: 128  # in megabytes, the default is 8192. Pods'
                            # memory request will be adjusted automatically.
      priorityClassName: critical-monitoring
      persistence:
        enabled: true
        storageSize: 2Gi  # Will be used to "extend" the `allocatedMemory`.
        storageClass: null
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: memcached-chunks-cache
            topologyKey: kubernetes.io/hostname
    resultsCache:
      replicas: 1
      allocatedMemory: 64  # in megabytes, the default is 1024. Pods'
                           # memory request will be adjusted automatically.
      priorityClassName: critical-monitoring
      persistence:
        enabled: true
        storageSize: 2Gi  # Will be used to "extend" the `allocatedMemory`.
        storageClass: null
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/component: memcached-results-cache
            topologyKey: kubernetes.io/hostname
    lokiCanary:
      image:
        repository: grafana/loki-canary
        tag: 3.5.1
        digest: null
      resources: {}
    gateway:
      replicas: 1
      verboseLogging: false
      image:
        repository: nginxinc/nginx-unprivileged
        tag: 1.28-alpine
        digest: null
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: gateway
              topologyKey: kubernetes.io/hostname
              matchLabelKeys:
              - pod-template-hash
      resources: {}
    sidecar:
      image:
        # The registry must be included in the "repository" value
        # ("docker.io" for example), because the chart ignores the
        # value of "global.image.registry" for sidecars.
        repository: docker.io/kiwigrid/k8s-sidecar
        tag: 1.30.3
        sha: ""
      resources: {}
    memcached:
      image:
        repository: memcached
        tag: 1.6.38-alpine
    memcachedExporter:
      image:
        repository: prom/memcached-exporter
        tag: v0.15.3
      resources: {}
    test:
      enabled: false
