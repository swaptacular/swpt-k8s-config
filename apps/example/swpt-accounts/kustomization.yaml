apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# The name of the namespace could be changed. However, the name of the
# namespace and the name of the directory that contains this
# `kustomization.yaml` file MUST always be the same!
namespace: swpt-accounts

secretGenerator:
- name: spilo-env-secret
  files:
  # A SOPS-encrypted file containing the secret used to encrypt the
  # WALG archive. Once created, the secret must not be changed. To
  # generate and SOPS-encrypt a random secret, you may use this
  # command:
  #
  # $ echo -n "$(openssl rand -base64 32)" > spilo-walg-libsodium-key
  # $ sops encrypt spilo-walg-libsodium-key > spilo-walg-libsodium-key.encrypted
  # $ shred -z spilo-walg-libsodium-key
  # $ rm spilo-walg-libsodium-key
  - WALG_LIBSODIUM_KEY=secrets/spilo-walg-libsodium-key.encrypted

  # A SOPS-encrypted file containing the "password" for the S3
  # service. For the testing MinIO server, the password is
  # "console123". To SOPS-encrypt the password, you may use this
  # command:
  #
  # $ echo -n 'YOUR_SECRET_PASSWORD' > spilo-aws-secret-access-key
  # $ sops encrypt spilo-aws-secret-access-key > spilo-aws-secret-access-key.encrypted
  # $ shred -z spilo-aws-secret-access-key
  # $ rm spilo-aws-secret-access-key
  - AWS_SECRET_ACCESS_KEY=secrets/spilo-aws-secret-access-key.encrypted

  literals:
  # The "username" for the S3 service. For the testing MinIO server,
  # the username is "console".
  - AWS_ACCESS_KEY_ID=console

  # The endpoint for the S3 service.
  - AWS_ENDPOINT=https://minio.minio-tenant.svc.cluster.local:443

  # When Amazon S3 is used, this specifies the AWS region. In that
  # case, specifying `AWS_ENDPOINT` should not be necessary.
  - AWS_REGION=us-east-1

  # This specifies the trusted CA for the local MinIO S3 server. This
  # should not be necessary for "proper" S3 providers, and can be
  # safely removed.
  - WALG_S3_CA_CERT_FILE=/certs/minio/ca.crt

  options:
    disableNameSuffixHash: true
    labels:
      app.kubernetes.io/name: swpt-accounts
      application: spilo
      team: swpt

    # These annotations are used as sources for convenient Kustomize
    # replacements:
    #
    annotations:
      # For security reasons, by default pods are not allowed egress
      # traffic outside of the Kubernetes cluster. However, S3 backups
      # require initiating connections to S3 servers that are usually
      # not in the Kubernetes cluster. Here you must specify the range
      # of IPs which the "spilo" pods will be allowed to connect to.
      #
      kustomize_s3_ip_block: 10.0.0.0/8

  type: Opaque

- name: db-owner-credentials
  files:
  # A SOPS-encrypted file containing the password for the "db_owner"
  # user on all Postgres clusters which this application will create.
  # Once created, the password must not be changed. To generate and
  # SOPS-encrypt a random password, you may use this command:
  #
  # $ echo -n "$(openssl rand -hex 20)" > postgres-cluster-password
  # $ sops encrypt postgres-cluster-password > postgres-cluster-password.encrypted
  # $ shred -z postgres-cluster-password
  # $ rm postgres-cluster-password
  - password=secrets/postgres-cluster-password.encrypted

  literals:
  - username=db_owner

  options:
    disableNameSuffixHash: true
    labels:
      app.kubernetes.io/name: swpt-accounts
  type: Opaque

- name: server-certificate
  files:
  # The server certificate PEM file. This certificate will be used to
  # authenticate before peer nodes. On how to generate a server
  # certificate, check https://github.com/swaptacular/swpt_ca_scripts
  - server.crt=server.crt

  # A SOPS-encrypted file containing the private key for the server
  # certificate. This private key will be used to authenticate before
  # peer nodes. (See the previous comment.)
  #
  # To SOPS-encrypt the private key, you may use this command:
  #
  # $ sops encrypt server.key > server.key.encrypted
  # $ shred -z server.key
  # $ rm server.key
  - server.key=server.key.encrypted

  options:
    labels:
      app.kubernetes.io/name: swpt-accounts
  type: Opaque

- name: ssl-session-ticket-key
  files:
  # A SOPS-encrypted file containing the secret key for encrypting
  # SSL/TLS session tickets. To generate this file you may use the
  # following commands:
  #
  # $ openssl rand 80 > ssl-session-ticket-key
  # $ sops -e ssl-session-ticket-key > ssl-session-ticket-key.encrypted
  # $ shred -z ssl-session-ticket-key
  # $ rm ssl-session-ticket-key
  - key=secrets/ssl-session-ticket-key.encrypted

  options:
    disableNameSuffixHash: true
    labels:
      app.kubernetes.io/name: swpt-debtors
  type: Opaque

- name: git-ops-ssh-id
  files:
  - id_rsa=../swpt-nfs-server/secrets/id_rsa.encrypted
  - id_rsa.pub=../swpt-nfs-server/secrets/id_rsa.pub
  - id_rsa-cert.pub=../swpt-nfs-server/secrets/id_rsa-cert.pub
  options:
    disableNameSuffixHash: true
  type: Opaque

- name: regcreds
  files:
  - ".dockerconfigjson=../regcreds.json.encrypted"
  options:
    disableNameSuffixHash: true
  type: kubernetes.io/dockerconfigjson

configMapGenerator:
- name: accounting-authority-config

  literals:
  # The publicly known domain name of this Swaptacular node.
  - PUBLIC_HOSTNAME=swpt-tests-aa

  options:
    labels:
      app.kubernetes.io/name: swpt-accounts

    # These annotations are used as sources for convenient Kustomize
    # replacements:
    #
    annotations:
      # Configurations for the ACME certificate issuer. See the
      # "acme-cert-issuer" patch also.
      #
      kustomize_acme_cert_server: https://pebble.pebble.svc.cluster.local:14000/dir
      kustomize_acme_cert_email: epandurski@gmail.com

      # Kubernetes memory and CPU requests and limits, for various
      # containers.
      #
      kustomize_ingress_nginx_cpu_request: 1m
      kustomize_ingress_nginx_cpu_limit: 4000m
      kustomize_ingress_nginx_memory_request: 90Mi
      kustomize_ingress_nginx_memory_limit: 1Gi

- name: stomp-config
  literals:
  # The number of SMP messages that STOMP servers and STOMP clients
  # will buffer in memory.
  - SWPT_SERVER_BUFFER=100
  - SWPT_CLIENT_BUFFER=100

  # The number of worker threads that STOMP servers will use for file
  # access to the NFS server.
  - APP_FILE_READ_THREADS=10

  options:
    labels:
      app.kubernetes.io/name: swpt-accounts

    # These annotations are used as sources for convenient Kustomize
    # replacements:
    #
    annotations:
      # The static cluster IP address of the
      # "nfs-server.swpt-nfs-server.svc.cluster.local" service.
      kustomize_nfs_service_cluster_ip: 10.96.0.4

      # Each Swaptacular node runs its own STOMP server. This is the
      # port at which the node's STOMP server will be running.
      kustomize_stomp_server_port: "1234"

      # Kubernetes memory and CPU requests and limits, for the STOMP
      # server containers.
      #
      kustomize_stomp_server_cpu_request: 1m
      kustomize_stomp_server_cpu_limit: 1000m
      kustomize_stomp_server_memory_request: 24Mi
      kustomize_stomp_server_memory_limit: 1Gi

- name: apiproxy-config
  files:
  # The configuration file for the "apiproxy".
  #
  # Each line in the configuration file should start with a route
  # specifier, followed by at least one space, followed by a web
  # server URL (only "http://" server URLs are supported). Route
  # specifiers consist of zero or more 0s or 1s, separated by dots,
  # ending with a hash symbol ("#"). An example configuration file:
  #
  # 0.# http://http-cache-0
  # 1.# http://http-cache-1
  #
  # or, for a sigle server:
  #
  # # http://http-cache
  #
  # Note that the route specifiers in the configuration file must
  # cover all possible bit masks. For example, the following
  # configuration file is invalid:
  #
  # 0.# http://http-cache-0
  #
  # because it does not cover sharding keys starting with a
  # binary "1".
  - apiproxy.conf=apiproxy.conf

  options:
    labels:
      app.kubernetes.io/name: swpt-accounts

    # These annotations are used as sources for convenient Kustomize
    # replacements:
    #
    annotations:
      # Kubernetes memory and CPU requests and limits, for the
      # apiproxy server containers.
      #
      kustomize_apiproxy_cpu_request: 1m
      kustomize_apiproxy_cpu_limit: 1000m
      kustomize_apiproxy_memory_request: 19Mi
      kustomize_apiproxy_memory_limit: 1Gi

- name: git-ops-config
  literals:
  - GIT_SERVER=git-server.simple-git-server.svc.cluster.local
  - GIT_PORT=2222
  - GIT_REPOSITORY_PATH=/srv/git/fluxcd.git
  options:
    disableNameSuffixHash: true

# The number of replicas for each component. One replica works fine,
# but to have high-availability, should be at least 2.
#
replicas:
- name: apiproxy
  count: 1
- name: stomp-server
  count: 1

# OCI images to use for the different containers:
#
images:
- name: swpt-accounts
  newName: ghcr.io/swaptacular/swpt_accounts
  digest: sha256:306139cd546b05a87e1357f48a480b344e05478f1d6e57b4e9bdbaeb3b86f86a
- name: swpt-apiproxy
  newName: ghcr.io/swaptacular/swpt_apiproxy
  digest: sha256:7a396d200c4c023f5cf46a3dc45d202917df4ef184e33580c3177a70c4a3675b
- name: swpt-stomp
  newName: ghcr.io/swaptacular/swpt_stomp
  digest: sha256:393fc048dad43fa95587de8d0f486a73427f8077fb0ad147c94c9dc3018ee7e3
- name: nginx
  newName: ghcr.io/swaptacular/nginx
  digest: sha256:65645c7bb6a0661892a8b03b89d0743208a18dd2f3f17a54ef4b76fb8e2f2a10
- name: swpt-nfs-server
  newName: ghcr.io/swaptacular/swpt_nfs_server
  digest: sha256:f3674f3a027e961d6c4be308094e1ec129859fda1aafac85e5920edb0c9bdb7f
- name: swpt-split-shard
  newName: ghcr.io/swaptacular/swpt_split_shard
  digest: sha256:0bf2a6820051d12cc72a8d4a46d2e5835cdbdfcc7c7fce13d2ffee59ddb93957

resources:
- ../../base/swpt-accounts/
- broker.yaml
- shards/
- node-data/peers/

patches:
# This patch configures the ingress-nginx controller.
#
- patch: |
    # Specifies the ingress-nginx controller's OCI image. You should
    # set image repository and image digest.
    - op: add
      path: /spec/values/controller/image/repository
      value: ghcr.io/swaptacular/ingress-nginx-controller
    - op: add
      path: /spec/values/controller/image/digest
      value: sha256:05890cb25d37aa5cfe086614104f798f55e1eeec8dda26d9fd6f6acf0e1554a0

    # Specifies the "kube-webhook-certgen" OCI image. This image is
    # used by the ingress-nginx controller to set up Kubernetes
    # admission webhooks for the resources managed by the controller.
    # You should set image repository and image digest.
    - op: add
      path: /spec/values/controller/admissionWebhooks/patch/image/repository
      value: ghcr.io/swaptacular/kube-webhook-certgen
    - op: add
      path: /spec/values/controller/admissionWebhooks/patch/image/digest
      value: sha256:7a38cf0f8480775baaee71ab519c7465fd1dfeac66c421f28f087786e631456e

    # Specifies the number of Nginx servers' replicas. One replica
    # works fine, but to have high-availability, should be at least 2.
    - op: replace
      path: /spec/values/controller/replicaCount
      value: 1

    # Configurations for the load balancer service.
    #
    # When the external traffic policy is "Cluster", the client IP
    # addresses will be unknown to the service. To preserve the client
    # IPs, the traffic policy should be set to "Local", in which case
    # most providers will preserve the client IPs. However, for some
    # providers this will not be enough. Digital Ocean, for example,
    # requires you to also add this annotation to the service:
    # `service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol:
    # "true"`, and then change/configure the your ingress-nginx
    # servers to use "Proxy protocol" (see the next comment, also see
    # https://seriousben.com/posts/2020-02-exploring-the-proxy-protocol/).
    #
    - op: replace
      path: /spec/values/controller/service
      value:
        externalTrafficPolicy: Cluster
        annotations:
          service.beta.kubernetes.io/do-loadbalancer-enable-proxy-protocol: "false"

    # Sets the correct reference to the stomp-server service. The
    # format is "namespace/service-name:port[:PROXY]". Where:
    #
    # * "namespace" must be the same as the current namespace. That is:
    #   the name of the directory that contains this `kustomization.yaml`
    #   file;
    #
    # * "service-name" must be "stomp-server"; "port" must be "stomp";
    #
    # * If the optional ":PROXY" is appended to the value, this means that
    #   PROXY protocol parsing will be applied to the incoming STOMP
    #   connections.
    - op: replace
      path: /spec/values/tcp/1234
      value: swpt-accounts/stomp-server:stomp

    # Enables or disables the PROXY protocol parsing for the incoming
    # HTTP and HTTPS connections. To enable PROXY protocol parsing,
    # set the value to "true". To disable PROXY protocol parsing, set
    # the value to "false".
    - op: replace
      path: /spec/values/controller/config/use-proxy-protocol
      value: "false"

  target:
    group: helm.toolkit.fluxcd.io
    version: v2
    kind: HelmRelease
    name: ingress-nginx

- path: ../../base/swpt-accounts/patches/broker.yaml
  target:
    group: rabbitmq.com
    kind: RabbitmqCluster
    name: broker
    version: v1beta1

replacements:
- path: ../../base/swpt-accounts/replacements/nfs-cluster-ip.yaml
- path: ../../base/swpt-accounts/replacements/nfs-pv-name.yaml
- path: ../../base/swpt-accounts/replacements/stomp-server-port.yaml
- path: ../../base/swpt-accounts/replacements/apiproxy-cpu-limit.yaml
- path: ../../base/swpt-accounts/replacements/apiproxy-cpu-request.yaml
- path: ../../base/swpt-accounts/replacements/apiproxy-memory-limit.yaml
- path: ../../base/swpt-accounts/replacements/apiproxy-memory-request.yaml
- path: ../../base/swpt-accounts/replacements/stomp-server-cpu-limit.yaml
- path: ../../base/swpt-accounts/replacements/stomp-server-cpu-request.yaml
- path: ../../base/swpt-accounts/replacements/stomp-server-memory-limit.yaml
- path: ../../base/swpt-accounts/replacements/stomp-server-memory-request.yaml
- path: ../../base/swpt-accounts/replacements/ingress-nginx-cpu-limit.yaml
- path: ../../base/swpt-accounts/replacements/ingress-nginx-cpu-request.yaml
- path: ../../base/swpt-accounts/replacements/ingress-nginx-memory-limit.yaml
- path: ../../base/swpt-accounts/replacements/ingress-nginx-memory-request.yaml
- path: ../../base/swpt-accounts/replacements/public-hostname.yaml
- path: ../../base/swpt-accounts/replacements/acme-cert-server.yaml
- path: ../../base/swpt-accounts/replacements/acme-cert-email.yaml
- path: ../../base/swpt-accounts/replacements/s3-ip-block.yaml
- path: ../../base/swpt-accounts/replacements/ingress-nginx-namespace.yaml
